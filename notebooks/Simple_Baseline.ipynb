{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "pd.set_option('max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cutoff = 0.2\n",
    "neg_cutoff = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import validation data for running a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = np.load('../data/interim/val_data.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload columns from raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/Tweets.csv')\n",
    "cols = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dev DataFrame\n",
    "val = pd.DataFrame(val, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1464, 15)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply textblob.sentiment method to get sentiment from text column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TextBlob.sentiment` returns a tuple of (*polarity*, *subjectivity*).  Not worried about the subjectivity right now so we'll just pull the polarity score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "val['polarity'] = val.text.apply(lambda x: TextBlob(x).sentiment[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on descriptive stats, bin data based on IQR 0 >= neg, and 0.2 <= pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1464.000000\n",
       "mean        0.052598\n",
       "std         0.316773\n",
       "min        -1.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.200000\n",
       "max         1.000000\n",
       "Name: polarity, dtype: float64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val['polarity'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create labels of target data based on polarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_to_label(polarity_score):\n",
    "    if polarity_score >= pos_cutoff:\n",
    "        return 2\n",
    "    elif polarity_score > neg_cutoff:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "val['labels'] = val['polarity'].apply(lambda x: polarity_to_label(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create true target labels based on airline sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_label(airline_sentiment):\n",
    "    if airline_sentiment == 'positive':\n",
    "        return 2\n",
    "    elif airline_sentiment == 'neutral':\n",
    "        return 1\n",
    "    elif airline_sentiment == 'negative':\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "val['y_label'] = val['airline_sentiment'].apply(lambda x: sent_to_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.rename({'labels':'predicted', 'y_label':'actual'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and interpret Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.73      0.72       913\n",
      "     neutral       0.22      0.10      0.14       304\n",
      "    positive       0.41      0.65      0.50       247\n",
      "\n",
      "    accuracy                           0.58      1464\n",
      "   macro avg       0.45      0.49      0.45      1464\n",
      "weighted avg       0.56      0.58      0.56      1464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val['actual'], val['predicted'], target_names=['negative', 'neutral', 'positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad$ Overall accuracy for a naive model isn't horrible, it's almost twice as good as random guessing, which would be about 1 out of 3 correct or 33%.  This model captured 58%.  Not surprisingly, the precision for negative responses was highest (there were a lot of negative responses), while the worst scores were in the neutral category.  \n",
    "$\\qquad$  We'll need a more nuanced model to capture the \"neutral\" sentiment.  Our recall for neutral is an abysymal 10%, meaning we are systematically misclassifying this category.  The main driver, of course, for this model are the pos and neg cutoff thresholds that we defined earlier.  No doubt, the model predictions will likely differ greatly depending on the thresholds that we define. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[662,  86, 165],\n",
       "       [207,  30,  67],\n",
       "       [ 65,  21, 161]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(val['actual'], val['predicted'], labels=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter_sent",
   "language": "python",
   "name": "twitter_sent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
